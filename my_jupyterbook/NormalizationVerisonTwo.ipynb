{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disease Prediction Model Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell imports all the necessary libraries and modules required for the following tasks:\n",
    "- **Data Handling**: Libraries such as `NumPy` and `pandas` are used for data manipulation and analysis.\n",
    "- **Data Visualization**: `matplotlib` and `seaborn` are included for creating insightful visualizations.\n",
    "- **Machine Learning**: `scikit-learn`, `XGBoost`, and `imblearn` are used for building, optimizing, and evaluating machine learning models.\n",
    "- **Pipeline and Feature Engineering**: Tools from `scikit-learn` for preprocessing, feature selection, and pipeline creation.\n",
    "- **Model Evaluation**: Metrics such as accuracy, F1-score, confusion matrix, and classification reports to assess model performance.\n",
    "- **Dataset Profiling**: `ydata_profiling` is used to generate comprehensive profiling reports for exploratory data analysis.\n",
    "- **Model Deployment**: `mlflow` and `joblib` support model tracking, saving, and deployment.\n",
    "- **Database Operations**: The `sqlite3` library is used for interacting with SQLite databases.\n",
    "- **Inline Plotting**: `%matplotlib inline` ensures all visualizations are displayed inline in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectFromModel\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from ydata_profiling import ProfileReport  \n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import joblib\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "from xgboost import XGBClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell is responsible for loading the diabetes dataset into the notebook for analysis. It uses the `pandas` library to read the data from a CSV file named `diabetes.csv` located in the current directory. The dataset is stored in a DataFrame called `beforeDataClean`.\n",
    "\n",
    "This step is critical for initializing the data analysis process and serves as the foundation for all subsequent data manipulation and modeling tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\diabetes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m beforeDataClean \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdiabetes.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Windows\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Windows\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Windows\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Windows\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Windows\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\diabetes.csv'"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "beforeDataClean = pd.read_csv(r\".\\diabetes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Dataset Profiling**:\n",
    "   - Uses `ProfileReport` from the `ydata_profiling` library to generate a comprehensive profiling report for the `beforeDataClean` dataset.\n",
    "   - The report provides insights into dataset structure, data types, missing values, and summary statistics.\n",
    "   - The output is displayed directly within the notebook as an iframe.\n",
    "\n",
    "2. **Correlation Analysis**:\n",
    "   - Computes the correlation matrix for all numerical columns in the dataset.\n",
    "   - Visualizes the correlation matrix using a heatmap created with `seaborn`, providing a clear representation of relationships between numerical features.\n",
    "   - Customizes the heatmap with annotations, a color map (\"coolwarm\"), and a title for improved readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate Profile Report\n",
    "print(\"Generating profile report...\")\n",
    "profile = ProfileReport(beforeDataClean, title=\"Diabetes Dataset Profiling Report\")\n",
    "profile.to_notebook_iframe()\n",
    "# Compute the correlation matrix for numerical columns\n",
    "numerical_columns = beforeDataClean.select_dtypes(include=['number'])\n",
    "correlation_matrix = numerical_columns.corr()\n",
    "plt.figure(figsize=(10, 8))  # Set the figure size\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "![Correlation Matrix](Heatmatrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell focuses on cleaning the dataset to prepare it for further analysis and modeling. The following steps are performed:\n",
    "\n",
    "1. **Create a Copy**:\n",
    "   - A deep copy of the `beforeDataClean` DataFrame is created to avoid modifying the original dataset. The copy is stored in `afterDataClean`.\n",
    "\n",
    "2. **Replace Invalid Values**:\n",
    "   - Columns `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, and `BMI` are checked for invalid entries (e.g., zeros).\n",
    "   - Invalid entries are replaced with `NaN` to handle them appropriately during the cleaning process.\n",
    "\n",
    "3. **Fill Missing Values**:\n",
    "   - Missing values in `Glucose` and `BloodPressure` are filled with their respective column means.\n",
    "   - Missing values in `SkinThickness`, `Insulin`, and `BMI` are filled with their respective column medians for robustness against outliers.\n",
    "\n",
    "4. **Check for Remaining Missing Values**:\n",
    "   - Prints the count of remaining `NaN` values to ensure the dataset is free of missing data.\n",
    "\n",
    "5. **Prepare Final Data**:\n",
    "   - The cleaned data is stored in the `data` DataFrame, which will be used in subsequent processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "afterDataClean = beforeDataClean.copy(deep=True)\n",
    "afterDataClean[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = (\n",
    "    afterDataClean[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.NaN))\n",
    "afterDataClean['Glucose'] = afterDataClean['Glucose'].fillna(round(afterDataClean['Glucose'].mean()))\n",
    "afterDataClean['BloodPressure'] = afterDataClean['BloodPressure'].fillna(round(afterDataClean['BloodPressure'].mean()))\n",
    "afterDataClean['SkinThickness'] = afterDataClean['SkinThickness'].fillna(round(afterDataClean['SkinThickness'].median()))\n",
    "afterDataClean['Insulin'] = afterDataClean['Insulin'].fillna(round(afterDataClean['Insulin'].median()))\n",
    "afterDataClean['BMI'] = afterDataClean['BMI'].fillna(round(afterDataClean['BMI'].median()))\n",
    "\n",
    "# Check for remaining NaN values\n",
    "print(\"Remaining NaN values:\", afterDataClean.isnull().sum())\n",
    "\n",
    "\n",
    "# Final data for processing\n",
    "data = afterDataClean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell manages database operations by establishing a connection, creating tables, inserting data, and retrieving it for analysis. The following key steps are performed:\n",
    "\n",
    "1. **Database Connection**:\n",
    "   - A `create_connection` function is defined to establish a connection to an SQLite database.\n",
    "   - Includes optional functionality to delete an existing database file before creating a new one.\n",
    "   - Ensures foreign key constraints are enabled.\n",
    "\n",
    "2. **Table Creation**:\n",
    "   - Three tables are created in the database:\n",
    "     - `Patient`: Stores basic patient information (e.g., `Pregnancies`, `Age`).\n",
    "     - `HealthMetrics`: Stores health-related metrics (e.g., `Glucose`, `BloodPressure`, `BMI`).\n",
    "     - `Outcome`: Stores the diabetes status of patients.\n",
    "   - The `create_table` function executes SQL statements to define the schema for each table.\n",
    "\n",
    "3. **Data Insertion**:\n",
    "   - Three separate functions insert data into the respective tables:\n",
    "     - `insert_patient_data`: Inserts patient demographic data.\n",
    "     - `insert_health_metrics`: Inserts health metrics data, linking it to patient IDs.\n",
    "     - `insert_outcome`: Inserts diabetes outcome data, also linked to patient IDs.\n",
    "   - Uses `conn.commit()` to ensure changes are saved to the database.\n",
    "\n",
    "4. **Data Retrieval**:\n",
    "   - Combines data from all three tables using SQL `JOIN` statements.\n",
    "   - Retrieves the consolidated dataset into a `pandas` DataFrame for further analysis.\n",
    "\n",
    "5. **Data Validation**:\n",
    "   - Displays the first few rows of the consolidated dataset and prints descriptive statistics to verify data integrity.\n",
    "\n",
    "6. **Database Closure**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Connection Functions\n",
    "def create_connection(db_file, delete_db=False):\n",
    "    if delete_db and os.path.exists(db_file):\n",
    "        os.remove(db_file)\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        conn.execute(\"PRAGMA foreign_keys = 1\")\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    return conn\n",
    "\n",
    "def create_table(conn, create_table_sql):\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(create_table_sql)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "\n",
    "# Create database and tables\n",
    "dbConnection = create_connection(\"normalizedData.db\", True)\n",
    "\n",
    "patientTable = '''CREATE TABLE Patient (\n",
    "    PatientID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    Pregnancies INTEGER,\n",
    "    Age INT\n",
    ");'''\n",
    "healthMatrixTable = '''CREATE TABLE HealthMetrics (\n",
    "    PatientID INTEGER,\n",
    "    Glucose INTEGER,\n",
    "    BloodPressure INTEGER,\n",
    "    SkinThickness INTEGER,\n",
    "    Insulin INTEGER,\n",
    "    BMI FLOAT,\n",
    "    DiabetesPedigreeFunction FLOAT,\n",
    "    FOREIGN KEY (PatientID) REFERENCES Patient(PatientID)\n",
    ");'''\n",
    "diseaseOutcomeTable = '''CREATE TABLE Outcome (\n",
    "    PatientID INTEGER,\n",
    "    DiabetesStatus INTEGER,\n",
    "    FOREIGN KEY (PatientID) REFERENCES Patient(PatientID)\n",
    ");'''\n",
    "\n",
    "create_table(dbConnection, patientTable)\n",
    "create_table(dbConnection, healthMatrixTable)\n",
    "create_table(dbConnection, diseaseOutcomeTable)\n",
    "\n",
    "# Insert data into tables\n",
    "def insert_patient_data(conn, data):\n",
    "    cursor = conn.cursor()\n",
    "    for _, row in data.iterrows():\n",
    "        cursor.execute(\n",
    "            \"INSERT INTO Patient (Pregnancies, Age) VALUES (?, ?)\",\n",
    "            (row[\"Pregnancies\"], row[\"Age\"])\n",
    "        )\n",
    "    conn.commit()\n",
    "\n",
    "def insert_health_metrics(conn, data):\n",
    "    cursor = conn.cursor()\n",
    "    patient_ids = cursor.execute(\"SELECT PatientID FROM Patient\").fetchall()\n",
    "    for _, (row, patient_id) in enumerate(zip(data.iterrows(), patient_ids)):\n",
    "        row = row[1]\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO HealthMetrics (\n",
    "                PatientID, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction\n",
    "            ) VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            (\n",
    "                patient_id[0],\n",
    "                row[\"Glucose\"],\n",
    "                row[\"BloodPressure\"],\n",
    "                row[\"SkinThickness\"],\n",
    "                row[\"Insulin\"],\n",
    "                row[\"BMI\"],\n",
    "                row[\"DiabetesPedigreeFunction\"]\n",
    "            )\n",
    "        )\n",
    "    conn.commit()\n",
    "\n",
    "def insert_outcome(conn, data):\n",
    "    cursor = conn.cursor()\n",
    "    patient_ids = cursor.execute(\"SELECT PatientID FROM Patient\").fetchall()\n",
    "    for _, (row, patient_id) in enumerate(zip(data.iterrows(), patient_ids)):\n",
    "        row = row[1]\n",
    "        cursor.execute(\n",
    "            \"INSERT INTO Outcome (PatientID, DiabetesStatus) VALUES (?, ?)\",\n",
    "            (patient_id[0], row[\"Outcome\"])\n",
    "        )\n",
    "    conn.commit()\n",
    "\n",
    "insert_patient_data(dbConnection, data)\n",
    "insert_health_metrics(dbConnection, data)\n",
    "insert_outcome(dbConnection, data)\n",
    "print(\"Data inserted successfully!\")\n",
    "\n",
    "# Load data from database\n",
    "dataFromTable = '''SELECT \n",
    "    p.PatientID, \n",
    "    p.Pregnancies, \n",
    "    p.Age, \n",
    "    hm.Glucose, \n",
    "    hm.BloodPressure, \n",
    "    hm.SkinThickness, \n",
    "    hm.Insulin, \n",
    "    hm.BMI, \n",
    "    hm.DiabetesPedigreeFunction, \n",
    "    o.DiabetesStatus AS Outcome\n",
    "FROM Patient p\n",
    "JOIN HealthMetrics hm ON p.PatientID = hm.PatientID\n",
    "JOIN Outcome o ON p.PatientID = o.PatientID;'''\n",
    "data = pd.read_sql_query(dataFromTable, dbConnection)\n",
    "print(data.head())\n",
    "dbConnection.close()\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell prepares the dataset for machine learning by performing feature engineering, addressing class imbalance, and defining preprocessing steps. The following operations are performed:\n",
    "\n",
    "1. **Feature and Target Separation**:\n",
    "   - Separates the dataset into features (`X`) and target variable (`y`), dropping irrelevant columns such as `PatientID`.\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - Creates new interaction features to capture relationships between variables:\n",
    "     - `Age_BMI`: The product of `Age` and `BMI`.\n",
    "     - `Glucose_BloodPressure`: The product of `Glucose` and `BloodPressure`.\n",
    "\n",
    "3. **Class Imbalance Handling**:\n",
    "   - Applies SMOTE (Synthetic Minority Oversampling Technique) to balance the class distribution in the target variable.\n",
    "   - Resamples the data to ensure equitable representation of all target classes, printing the resulting class distribution for validation.\n",
    "\n",
    "4. **Data Splitting**:\n",
    "   - Splits the resampled data into training and testing sets using an 80-20 split.\n",
    "\n",
    "5. **Pipeline Definition**:\n",
    "   - Defines key preprocessing steps for a machine learning pipeline:\n",
    "     - **Scaler**: Standardizes the data using `StandardScaler` for uniform scaling.\n",
    "     - **PCA**: Reduces dimensionality while retaining 95% of the variance.\n",
    "     - **Feature Selector**: Removes low-variance features using `VarianceThreshold`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features and target\n",
    "X = data.drop(['Outcome','PatientID'], axis=1)\n",
    "y = data['Outcome']\n",
    "\n",
    "# Feature Engineering (Experiment #3)\n",
    "data['Age_BMI'] = data['Age'] * data['BMI']\n",
    "data['Glucose_BloodPressure'] = data['Glucose'] * data['BloodPressure']\n",
    "\n",
    "# Handle imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "print(\"Class distribution after SMOTE:\", np.bincount(y_res))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Pipeline Steps\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=0.95)\n",
    "feature_selector = VarianceThreshold(threshold=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell initializes several machine learning models and defines custom experiments to enhance predictive performance. The following components are included:\n",
    "\n",
    "1. **Standard Models**:\n",
    "   - **Logistic Regression (`log_reg`)**: A linear model for binary classification with a maximum iteration limit of 200.\n",
    "   - **XGBoost Classifier (`xgb`)**: An optimized gradient boosting implementation tuned for log loss.\n",
    "   - **Ridge Classifier (`ridge`)**: A linear classifier with regularization to prevent overfitting.\n",
    "   - **Random Forest Classifier (`random_forest`)**: An ensemble model with 200 estimators and a maximum depth of 10.\n",
    "   - **Gradient Boosting Classifier (`gb`)**: A boosting model for improved predictive accuracy.\n",
    "\n",
    "2. **Custom Experiments**:\n",
    "   - **Stacking Classifier (`stacking_clf`)**:\n",
    "     - Combines predictions from `RandomForest` and `GradientBoosting` models.\n",
    "     - Uses `LogisticRegression` as the final estimator.\n",
    "     - Employs passthrough, allowing raw features to be included alongside base predictions for the final model.\n",
    "   - **Custom XGBoost (`custom_xgb`)**:\n",
    "     - Extends XGBoost with 300 estimators, a maximum depth of 15, and a learning rate of 0.05 to improve model performance on complex datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "log_reg = LogisticRegression(max_iter=200, random_state=42)\n",
    "xgb = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "ridge = RidgeClassifier(random_state=42)\n",
    "random_forest = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Custom Experiments (Experiment #6 and #7)\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[('rf', random_forest), ('gb', gb)], final_estimator=LogisticRegression(),\n",
    "    passthrough=True\n",
    ")\n",
    "custom_xgb = XGBClassifier(n_estimators=300, max_depth=15, learning_rate=0.05, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell defines preprocessing and classification pipelines for various machine learning models. Each pipeline standardizes the data, applies feature selection, reduces dimensionality, and fits a specific classifier. The following components are included in each pipeline:\n",
    "\n",
    "1. **Preprocessing Steps**:\n",
    "   - **Scaler**: Standardizes the features using `StandardScaler` to ensure uniform scaling.\n",
    "   - **Feature Selector**: Removes low-variance features with `VarianceThreshold`.\n",
    "   - **PCA**: Reduces dimensionality by retaining 95% of the variance, optimizing computational efficiency.\n",
    "\n",
    "2. **Classifiers**:\n",
    "   - `Logistic Regression`: A linear model for binary classification.\n",
    "   - `Ridge Classifier`: A regularized linear model for robustness against overfitting.\n",
    "   - `Random Forest`: An ensemble method using decision trees for classification.\n",
    "   - `XGBClassifier`: A high-performance gradient boosting algorithm.\n",
    "   - `Gradient Boosting`: Another boosting algorithm optimized for accuracy.\n",
    "   - `Stacking`: Combines predictions from `Random Forest` and `Gradient Boosting` with `Logistic Regression` as the final estimator.\n",
    "   - `Custom XGB`: An extended XGBoost model with tuned hyperparameters for enhanced performance.\n",
    "\n",
    "3. **Pipeline Dictionary**:\n",
    "   - Stores all pipelines in a dictionary named `pipelines`, allowing for easy iteration and evaluation of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pipelines\n",
    "pipelines = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('feature_selector', feature_selector),\n",
    "        ('pca', pca),\n",
    "        ('classifier', log_reg)\n",
    "    ]),\n",
    "    'Ridge Classifier': Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('feature_selector', feature_selector),\n",
    "        ('pca', pca),\n",
    "        ('classifier', ridge)\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('feature_selector', feature_selector),\n",
    "        ('pca', pca),\n",
    "        ('classifier', random_forest)\n",
    "    ]),\n",
    "    'XGBClassifier': Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('feature_selector', feature_selector),\n",
    "        ('pca', pca),\n",
    "        ('classifier', xgb)\n",
    "    ]),\n",
    "    'Gradient Boosting': Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('feature_selector', feature_selector),\n",
    "        ('pca', pca),\n",
    "        ('classifier', gb)\n",
    "    ]),\n",
    "    'Stacking': Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('feature_selector', feature_selector),\n",
    "        ('pca', pca),\n",
    "        ('classifier', stacking_clf)\n",
    "    ]),\n",
    "    'Custom XGB': Pipeline([\n",
    "        ('scaler', scaler),\n",
    "        ('feature_selector', feature_selector),\n",
    "        ('pca', pca),\n",
    "        ('classifier', custom_xgb)\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell initializes the setup for evaluating machine learning models using cross-validation. The following steps are performed:\n",
    "\n",
    "1. **Results Dictionary**:\n",
    "   - Creates an empty dictionary named `results` to store the evaluation metrics for each model.\n",
    "\n",
    "2. **Cross-Validation Strategy**:\n",
    "   - Utilizes `StratifiedKFold` to split the dataset into 10 folds while preserving the class distribution in each fold.\n",
    "   - Parameters:\n",
    "     - `n_splits=10`: Divides the dataset into 10 folds for training and validation.\n",
    "     - `shuffle=True`: Shuffles the data before splitting to ensure randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **MLflow Configuration**:\n",
    "   - Sets up the MLflow tracking server using the provided URI, username, and password.\n",
    "   - Defines the experiment name as `\"diseasePredictionSystemModel\"` to organize logged runs.\n",
    "\n",
    "2. **Cross-Validation and Evaluation**:\n",
    "   - Iterates through each pipeline in the `pipelines` dictionary.\n",
    "   - Performs cross-validation on the training set (`X_train`, `y_train`) using `StratifiedKFold` and calculates F1 scores.\n",
    "   - Trains the pipeline on the full training set and evaluates it on the test set (`X_test`, `y_test`).\n",
    "   - Computes various performance metrics, including:\n",
    "     - Mean and standard deviation of cross-validation F1 scores.\n",
    "     - Accuracy and F1 score on the test set.\n",
    "     - Confusion matrix components: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
    "\n",
    "3. **Results Storage**:\n",
    "   - Stores the evaluation metrics for each model in the `results` dictionary for comparison.\n",
    "\n",
    "4. **MLflow Logging**:\n",
    "   - Logs key metrics such as mean cross-validation F1 score, accuracy, and confusion matrix components to MLflow.\n",
    "   - Captures the model's signature (input-output schema) using `infer_signature`.\n",
    "   - Logs the trained pipeline to MLflow as an artifact for reproducibility and deployment.\n",
    "   - Assigns a registered model name based on the pipeline's key in the dictionary.\n",
    "\n",
    "5. **Model Management**:\n",
    "   - MLflow handles the storage and organization of trained models, allowing easy comparison and deployment in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_URI = \"https://dagshub.com/s3akash/diseasePredictionSystemModel.mlflow\"\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 's3akash'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '4d779774d517aa973e2858c5d93af0e87a4ca457'\n",
    "mlflow.set_tracking_uri(uri=MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(\"diseasePredictionSystemModel\")\n",
    "for name, pipeline in pipelines.items():\n",
    "    cv_f1_scores = cross_val_score(pipeline, X_train, y_train, cv=kfold, scoring='f1')\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    results[name] = {\n",
    "        'Mean CV F1 Score': cv_f1_scores.mean(),\n",
    "        'Std CV F1 Score': cv_f1_scores.std(),\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred),\n",
    "        'Confusion Matrix': cm,\n",
    "        'TP': cm[1, 1],\n",
    "        'TN': cm[0, 0],\n",
    "        'FP': cm[0, 1],\n",
    "        'FN': cm[1, 0]\n",
    "    }\n",
    "# Log metrics\n",
    "    mlflow.log_metric(\"mean_cv_f1_score\", cv_f1_scores.mean())\n",
    "    mlflow.log_metric(\"std_cv_f1_score\", cv_f1_scores.std())\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "    mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred))\n",
    "    mlflow.log_metric(\"true_positives\", cm[1, 1])\n",
    "    mlflow.log_metric(\"true_negatives\", cm[0, 0])\n",
    "    mlflow.log_metric(\"false_positives\", cm[0, 1])\n",
    "    mlflow.log_metric(\"false_negatives\", cm[1, 0])\n",
    "\n",
    "    signature = infer_signature(X_train, pipeline.predict(X_train))\n",
    "        \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=pipeline,\n",
    "        artifact_path=\"model\",\n",
    "        signature=signature,\n",
    "        input_example=X_train,\n",
    "        registered_model_name=name,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell visualizes the performance of various machine learning models, identifies the best-performing model, and saves it for future use. The following steps are performed:\n",
    "\n",
    "1. **Results Conversion**:\n",
    "   - Converts the `results` dictionary into a pandas DataFrame for better organization and analysis.\n",
    "   - Transposes the DataFrame to align models as rows and metrics as columns for easier interpretation.\n",
    "\n",
    "2. **F1-Score Visualization**:\n",
    "   - Creates a bar plot comparing the \"Mean CV F1 Score\" and \"F1 Score\" for each model.\n",
    "   - Customizes the plot with:\n",
    "     - A title: \"F1 Score Comparison Across Models.\"\n",
    "     - Labels for axes: \"Models\" (x-axis) and \"F1 Score\" (y-axis).\n",
    "     - Rotated x-axis labels for better readability.\n",
    "   - Displays the plot to provide a clear comparison of model performance.\n",
    "\n",
    "3. **Best Model Identification**:\n",
    "   - Determines the best-performing model based on the highest accuracy score in the results DataFrame.\n",
    "   - Stores the name of the best model for subsequent steps.\n",
    "\n",
    "4. **Model Saving**:\n",
    "   - Saves the pipeline of the best-performing model as a `.pkl` file (`disease_prediction_model2.pkl`) using `joblib`.\n",
    "   - Prints the name of the best model for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate F1-score plot\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n",
    "results_df[['Mean CV F1 Score', 'F1 Score']].plot(kind='bar', figsize=(12, 8))\n",
    "plt.title('F1 Score Comparison Across Models')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Save best model\n",
    "best_model_name = results_df['Accuracy'].idxmax()\n",
    "joblib.dump(pipelines[best_model_name], './disease_prediction_model2.pkl')\n",
    "print(f\"Best Model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Test Model Function**:\n",
    "   - `test_model`: A function to evaluate the model on individual test cases.\n",
    "   - Key steps:\n",
    "     - Adds feature-engineered columns (`Age_BMI` and `Glucose_BloodPressure`) to match the training set.\n",
    "     - Reorders columns in the test case to align with the model's expected input structure.\n",
    "     - Prints the test case input and the model's prediction for better interpretability.\n",
    "\n",
    "2. **Example Test Cases**:\n",
    "   - Five test cases are created as pandas DataFrames, each with the following features:\n",
    "     - `Pregnancies`, `Age`, `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`, and `DiabetesPedigreeFunction`.\n",
    "   - These test cases represent diverse scenarios to evaluate the model's prediction capability.\n",
    "\n",
    "3. **Best Model Evaluation**:\n",
    "   - Retrieves the best-performing pipeline from the `pipelines` dictionary using `best_model_name`.\n",
    "   - Uses the `test_model` function to predict outcomes for the defined test cases.\n",
    "\n",
    "4. **Output**:\n",
    "   - Displays the input values and predictions for each test case.\n",
    "   - Helps validate the model's performance on unseen data and provides insights into how feature engineering affects predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cases\n",
    "def test_model(model_pipeline, test_cases, feature_columns):\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        # Add feature engineering\n",
    "        test_case['Age_BMI'] = test_case['Age'] * test_case['BMI']\n",
    "        test_case['Glucose_BloodPressure'] = test_case['Glucose'] * test_case['BloodPressure']\n",
    "        # Reorder columns to match training\n",
    "        test_case = test_case[feature_columns]\n",
    "        print(test_case)\n",
    "        prediction = model_pipeline.predict(test_case)\n",
    "        print(f\"Test Case {i+1}: Input: {test_case.values.flatten().tolist()}, Prediction: {prediction[0]}\")\n",
    "\n",
    "# Example Test Cases\n",
    "test_case_1 = pd.DataFrame({\n",
    "    'Pregnancies': [1],\n",
    "    'Age': [22],\n",
    "    'Glucose': [85],\n",
    "    'BloodPressure': [70],\n",
    "    'SkinThickness': [20],\n",
    "    'Insulin': [50],\n",
    "    'BMI': [25.5],\n",
    "    'DiabetesPedigreeFunction': [0.201]\n",
    "})\n",
    "\n",
    "test_case_2 = pd.DataFrame({\n",
    "    'Pregnancies': [5],\n",
    "    'Age': [45],\n",
    "    'Glucose': [165],\n",
    "    'BloodPressure': [85],\n",
    "    'SkinThickness': [40],\n",
    "    'Insulin': [150],\n",
    "    'BMI': [35.0],\n",
    "    'DiabetesPedigreeFunction': [0.543]\n",
    "})\n",
    "\n",
    "test_case_3 = pd.DataFrame({\n",
    "    'Pregnancies': [3],\n",
    "    'Age': [33],\n",
    "    'Glucose': [110],\n",
    "    'BloodPressure': [70],\n",
    "    'SkinThickness': [25],\n",
    "    'Insulin': [90],\n",
    "    'BMI': [28.0],\n",
    "    'DiabetesPedigreeFunction': [0.350]\n",
    "})\n",
    "\n",
    "test_case_4 = pd.DataFrame({\n",
    "    'Pregnancies': [0],\n",
    "    'Age': [19],\n",
    "    'Glucose': [75],\n",
    "    'BloodPressure': [60],\n",
    "    'SkinThickness': [18],\n",
    "    'Insulin': [40],\n",
    "    'BMI': [22.0],\n",
    "    'DiabetesPedigreeFunction': [0.150]\n",
    "})\n",
    "\n",
    "test_case_5 = pd.DataFrame({\n",
    "    'Pregnancies': [8],\n",
    "    'Age': [50],\n",
    "    'Glucose': [180],\n",
    "    'BloodPressure': [90],\n",
    "    'SkinThickness': [45],\n",
    "    'Insulin': [230],\n",
    "    'BMI': [40.0],\n",
    "    'DiabetesPedigreeFunction': [0.750]\n",
    "})\n",
    "\n",
    "test_cases = [test_case_1, test_case_2, test_case_3, test_case_4, test_case_5]\n",
    "\n",
    "# Evaluate Best Model\n",
    "feature_columns = X_train.columns.tolist()\n",
    "best_pipeline = pipelines[best_model_name]\n",
    "test_model(best_pipeline, test_cases, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Define Test Case**:\n",
    "   - Creates a dictionary (`test_case`) with the following patient attributes:\n",
    "     - `Pregnancies`, `Age`, `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI`, and `DiabetesPedigreeFunction`.\n",
    "   - The test case represents a hypothetical patient scenario for prediction.\n",
    "\n",
    "2. **POST Request**:\n",
    "   - Sends the test case data as JSON to the API endpoint (`http://127.0.0.1:8000/predict`) using the `requests.post` method.\n",
    "   - The endpoint is expected to process the input data and return a prediction.\n",
    "\n",
    "3. **Response Handling**:\n",
    "   - Prints the HTTP status code to confirm the success or failure of the request.\n",
    "   - Prints the JSON response from the API, which contains the prediction result.\n",
    "\n",
    "**Use Case**:\n",
    "- This setup is ideal for integrating the machine learning model into a web application or service, allowing real-time predictions based on user-provided data.\n",
    "\n",
    "**Output Example**:\n",
    "- **Status Code**: Indicates the success of the API call (e.g., 200 for success).\n",
    "- **Response JSON**: Contains the prediction result, such as `{\"prediction\": 1}`, where the value represents the predicted disease outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define test payload\n",
    "test_case = {\n",
    "    \"Pregnancies\": 8,\n",
    "    \"Age\": 50,\n",
    "    \"Glucose\": 180,\n",
    "    \"BloodPressure\": 90,\n",
    "    \"SkinThickness\": 45,\n",
    "    \"Insulin\": 230,\n",
    "    \"BMI\": 40.0,\n",
    "    \"DiabetesPedigreeFunction\": 0.750\n",
    "}\n",
    "# Make POST request\n",
    "response = requests.post(\"http://127.0.0.1:8000/predict\", json=test_case)\n",
    "\n",
    "# Print response\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response JSON:\", response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
